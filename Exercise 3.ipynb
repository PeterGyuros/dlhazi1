{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Exercise 3 was the most diffcicult one for me, but I ended up with a concise and efficient code which I am very happy with.\n",
    "First of all we download the html code from the website using the requests libvrary and get() function. We store the raw html code in the html string. It will be translated to just characters and transferred to the content string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "        <title>The future of deep learning</title>\n",
      "        <meta charset=\"utf-8\" />\n",
      "        <link rel=\"stylesheet\" href=\"//blog.keras.io/theme/css/main.css\" type=\"text/css\" />\n",
      "        <link rel=\"stylesheet\" href=\"//blog.keras.io/theme/css/pygment.css\" type=\"text/css\" />\n",
      "\n",
      "        <link href=\"https://fonts.googleapis.com/css?family=Lato:400,700|Source+Sans+Pro:400,700|Inconsolata:400,700\" rel=\"stylesheet\" type=\"text/css\">\n",
      "        <link href=\"//blog.keras.io/\" type=\"application/atom+xml\" rel=\"alternate\" title=\"The Keras Blog ATOM Feed\" />\n",
      "\n",
      "\n",
      "        <!--[if IE]>\n",
      "                <script src=\"http://html5shiv.googlecode.com/svn/trunk/html5.js\"></script><![endif]-->\n",
      "\n",
      "        <!--[if lte IE 7]>\n",
      "                <link rel=\"stylesheet\" type=\"text/css\" media=\"all\" href=\"//blog.keras.io/css/ie.css\"/>\n",
      "                <script src=\"//blog.keras.io/js/IE8.js\" type=\"text/javascript\"></script><![endif]-->\n",
      "\n",
      "        <!--[if lt IE 7]>\n",
      "                <link rel=\"stylesheet\" type=\"text/css\" media=\"all\" href=\"//blog.keras.io/css/ie6.css\"/><![endif]-->\n",
      "\n",
      "</head>\n",
      "\n",
      "<body id=\"index\" class=\"home\">\n",
      "        <header id=\"banner\" class=\"body\">\n",
      "                <h1>\n",
      "                    <a href=\"//blog.keras.io/index.html\">The Keras Blog </a>\n",
      "                </h1>\n",
      "                <p id=\"side\">\n",
      "                    <a href=\"https://github.com/fchollet/keras\">Keras</a> is a Deep Learning library for Python, that is simple, modular, and extensible.\n",
      "                </p>\n",
      "                <nav><ul>\n",
      "                <li><a href=\"//blog.keras.io/\">Archives</a></li>\n",
      "                    <li >\n",
      "                        <a href=\"https://github.com/fchollet/keras\">Github</a>\n",
      "                    </li>\n",
      "                    <li >\n",
      "                        <a href=\"http://keras.io/\">Documentation</a>\n",
      "                    </li>\n",
      "                    <li >\n",
      "                        <a href=\"https://groups.google.com/forum/#!forum/keras-users\">Google Group</a>\n",
      "                    </li>\n",
      "                </ul></nav>\n",
      "        </header><!-- /#banner -->\n",
      "\n",
      "<section id=\"content\" class=\"body\">\n",
      "<article>\n",
      "        <header> <h1 class=\"entry-title\"><a href=\"the-future-of-deep-learning.html\"\n",
      "        rel=\"bookmark\" title=\"Permalink to The future of deep learning\">The future of deep learning</a></h1>  </header>\n",
      "        <div class=\"entry-content\">\n",
      "<footer class=\"post-info\">\n",
      "        <abbr class=\"published\" title=\"2017-07-18T00:00:00+02:00\">\n",
      "                Tue 18 July 2017\n",
      "        </abbr>\n",
      "\n",
      "        <address class=\"vcard author\">\n",
      "                By <a class=\"url fn\" href=\"https://twitter.com/fchollet\">Francois Chollet</a>\n",
      "        </address>\n",
      "<p>In <a href=\"//blog.keras.io/category/essays.html\">Essays</a>. </p>\n",
      "</p></footer><!-- /.post-info --><!-- /.post-info -->\n",
      "        <p>This post is adapted from Section 3 of Chapter 9 of my book, <a href=\"https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff\">Deep Learning with Python</a> (Manning Publications). </p>\n",
      "<p><a href=\"https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff\"><img alt=\"Deep learning with Python\" src=\"/img/deep_learning_with_python_cover_thumbnail.png\" /></a></p>\n",
      "<p>It is part of a series of two posts on the current limitations of deep learning, and its future. \n",
      "You can read the first part here: <a href=\"https://blog.keras.io/the-limitations-of-deep-learning.html\">The Limitations of Deep Learning</a>.</p>\n",
      "<hr />\n",
      "<p>Given what we know of how deep nets work, of their limitations, and of the current state of the research landscape, \n",
      "can we predict where things are headed in the medium term? Here are some purely personal thoughts. Note that I don't have a crystal ball, \n",
      "so a lot of what I anticipate might fail to become reality. This is a completely speculative post. \n",
      "I am sharing these predictions not because I expect them to be proven \n",
      "completely right in the future, but because they are interesting and actionable in the present.</p>\n",
      "<p>At a high-level, the main directions in which I see promise are:</p>\n",
      "<ul>\n",
      "<li>Models closer to general-purpose computer programs, built on top of far richer primitives than our current differentiable layersâthis \n",
      "is how we will get to <em>reasoning</em> and <em>abstraction</em>, the fundamental weakness of current models.</li>\n",
      "<li>New forms of learning that make the above possibleâallowing models to move away from just differentiable transforms.</li>\n",
      "<li>Models that require less involvement from human engineersâit shouldn't be your job to tune knobs endlessly.</li>\n",
      "<li>Greater, systematic reuse of previously learned features and architectures; meta-learning systems based on reusable and modular program \n",
      "subroutines.</li>\n",
      "</ul>\n",
      "<p>Additionally, do note that these considerations are not specific to the sort of supervised learning that has been the bread and butter of \n",
      "deep learning so farârather, they are applicable to any form of machine learning, including unsupervised, self-supervised, and \n",
      "reinforcement learning. It is not fundamentally important where your labels come from or what your training loop looks like; \n",
      "these different branches of machine learning are just different facets of a same construct.</p>\n",
      "<p>Let's dive in.</p>\n",
      "<h2>Models as programs</h2>\n",
      "<p>As we noted in our previous post, a necessary transformational development that we can expect in the field of machine learning is a move away \n",
      "from models that perform purely <em>pattern recognition</em> and can only achieve <em>local generalization</em>, towards models capable of <em>abstraction</em> and \n",
      "<em>reasoning</em>, that can achieve <em>extreme generalization</em>. Current AI programs that are capable of basic forms of reasoning are all hard-coded \n",
      "by human programmers: for instance, software that relies on search algorithms, graph manipulation, formal logic. \n",
      "In DeepMind's AlphaGo, for example, most of the \"intelligence\" on display is designed and hard-coded by expert programmers (e.g. Monte-Carlo tree search); \n",
      "learning from data only happens in specialized submodules (value networks and policy networks). \n",
      "But in the future, such AI systems may well be fully learned, with no human involvement.</p>\n",
      "<p>What could be the path to make this happen? Consider a well-known type of network: RNNs. \n",
      "Importantly, RNNs have slightly less limitations than feedforward networks. \n",
      "That is because RNNs are a bit more than a mere geometric transformation: they are geometric transformations <em>repeatedly applied inside a <code>for</code> \n",
      "loop</em>. The temporal <code>for</code> loop is itself hard-coded by human developers: it is a built-in assumption of the network. Naturally, RNNs are \n",
      "still extremely limited in what they can represent, primarily because each step they perform is still just a differentiable geometric \n",
      "transformation, and the way they carry information from step to step is via points in a continuous geometric space (state vectors). Now, \n",
      "imagine neural networks that would be \"augmented\" in a similar way with programming primitives such as <code>for</code> loopsâbut not just a single \n",
      "hard-coded <code>for</code> loop with a hard-coded geometric memory, rather, a large set of programming primitives that the model would be free to manipulate to expand its \n",
      "processing function, such as <code>if</code> branches, <code>while</code> statements, variable creation, disk storage for long-term memory, \n",
      "sorting operators, advanced datastructures like lists, graphs, and hashtables, and many more. \n",
      "The space of programs that such a network could represent would be far broader than what can be represented with current deep learning models, \n",
      "and some of these programs could achieve superior generalization power.</p>\n",
      "<p>In a word, we will move away from having on one hand \"hard-coded algorithmic intelligence\" (handcrafted software) and on the other hand \n",
      "\"learned geometric intelligence\" (deep learning). We will have instead a blend of formal algorithmic modules that provide <em>reasoning and \n",
      "abstraction</em> capabilities, and geometric modules that provide <em>informal intuition and pattern recognition</em> capabilities. The whole system would be \n",
      "learned with little or no human involvement.</p>\n",
      "<p>A related subfield of AI that I think may be about to take off in a big way is that of <em>program synthesis</em>, in particular neural program \n",
      "synthesis. Program synthesis consists in automatically generating simple programs, by using a search algorithm (possibly genetic search, as \n",
      "in genetic programming) to explore a large space of possible programs. The search stops when a program is found that matches the required \n",
      "specifications, often provided as a set of input-output pairs. As you can see, is it highly reminiscent of machine learning: given \n",
      "\"training data\" provided as input-output pairs, we find a \"program\" that matches inputs to outputs and can generalize to new inputs. The \n",
      "difference is that instead of learning parameter values in a hard-coded program (a neural network), \n",
      "we generate <em>source code</em> via a discrete search process.</p>\n",
      "<p>I would definitely expect this subfield to see a wave of renewed interest in the next few years. In particular, I would expect the \n",
      "emergence of a crossover subfield in-between deep learning and program synthesis, where we would not quite be generating programs in a \n",
      "general-purpose language, but rather, where we would be generating neural networks (geometric data processing flows) <em>augmented</em> with a \n",
      "rich set of algorithmic primitives, such as <code>for</code> loopsâand many others. This should be far more tractable and useful than directly \n",
      "generating source code, and it would dramatically expand the scope of problems that can be solved with machine learningâthe space of programs \n",
      "that we can generate automatically given appropriate training data. A blend of symbolic AI and geometric AI. \n",
      "Contemporary RNNs can be seen as a prehistoric ancestor to such hybrid algorithmic-geometric models.</p>\n",
      "<p><img alt=\"A learned program relying on both geometric (pattern recognition, intuition) and algorithmic (reasoning, search, memory) primitives.\" src=\"/img/future-of-dl/metalearning1.png\" /></p>\n",
      "<p><strong>Figure:</strong> <em>A learned program relying on both geometric primitives (pattern recognition, intuition) and algorithmic primitives (reasoning, search, memory).</em></p>\n",
      "<h2>Beyond backpropagation and differentiable layers</h2>\n",
      "<p>If machine learning models become more like programs, then they will mostly no longer be differentiableâcertainly, these programs will \n",
      "still leverage continuous geometric layers as subroutines, which will be differentiable, but the model as a whole would not be. As a \n",
      "result, using backpropagation to adjust weight values in a fixed, hard-coded network, cannot be the method of choice for training models in the \n",
      "futureâat least, it cannot be the whole story. We need to figure out to train non-differentiable systems efficiently. Current approaches \n",
      "include genetic algorithms, \"evolution strategies\", certain reinforcement learning methods, \n",
      "and ADMM (alternating direction method of multipliers). \n",
      "Naturally, gradient descent is not going anywhereâgradient information will always be useful for optimizing differentiable parametric functions. But our models \n",
      "will certainly become increasingly more ambitious than mere differentiable parametric functions, \n",
      "and thus their automatic development (the \"learning\" in \"machine learning\") will require more than backpropagation.</p>\n",
      "<p>Besides, backpropagation is end-to-end, which is a great thing for learning good chained transformations, but is rather computationally \n",
      "inefficient since it doesn't fully leverage the modularity of deep networks. To make something more efficient, there is one universal \n",
      "recipe: introduce modularity and hierarchy. So we can make backprop itself more efficient by introducing decoupled training modules with some \n",
      "synchronization mechanism between them, organized in a hierarchical fashion. This strategy is somewhat reflected in DeepMind's recent work \n",
      "on \"synthetic gradients\". I would expect more more work along these lines in the near future.</p>\n",
      "<p>One can imagine a future where models that would be globally non-differentiable (but would feature differentiable parts) would be \n",
      "trainedâgrownâusing an efficient search process that would not leverage gradients, while the differentiable parts would be trained even faster \n",
      "by taking advantage of gradients using some more efficient version of backpropagation.</p>\n",
      "<h2>Automated machine learning</h2>\n",
      "<p>In the future, model architectures will be learned, rather than handcrafted by engineer-artisans. Learning architectures automatically goes \n",
      "hand in hand with the use of richer sets of primitives and program-like machine learning models.</p>\n",
      "<p>Currently, most of the job of a deep learning engineer consists in munging data with Python scripts, then lengthily tuning the architecture \n",
      "and hyperparameters of a deep network to get a working modelâor even, to get to a state-of-the-art model, if the engineer is so \n",
      "ambitious. Needless to say, that is not an optimal setup. But AI can help there too. Unfortunately, the data munging part is tough to \n",
      "automate, since it often requires domain knowledge as well as a clear high-level understanding of what the engineer wants to achieve. \n",
      "Hyperparameter tuning, however, is a simple search procedure, and we already know what the engineer wants to achieve in this case: it is \n",
      "defined by the loss function of the network being tuned. It is already common practice to set up basic \"AutoML\" systems that will take care \n",
      "of most of the model knob tuning. I even set up my own years ago to win Kaggle competitions.</p>\n",
      "<p>At the most basic level, such a system would simply tune the number of layers in a stack, their order, and the number of units or filters \n",
      "in each layer. This is commonly done with libraries such as Hyperopt, which we discussed in Chapter 7 \n",
      "(Note: of <a href=\"https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;a_bid=76564dff\">Deep Learning with Python</a>). \n",
      "But we can also be far more \n",
      "ambitious, and attempt to learn an appropriate architecture from scratch, with as few constraints as possible. This is possible via \n",
      "reinforcement learning, for instance, or genetic algorithms.</p>\n",
      "<p>Another important AutoML direction is to learn model architecture jointly with model weights. Because training a new model from scratch \n",
      "every time we try a slightly different architecture is tremendously inefficient, a truly powerful AutoML system would manage to evolve \n",
      "architectures at the same time as the features of the model are being tuned via backprop on the training data, thus eliminating all computational redundancy. \n",
      "Such approaches are already starting to emerge as I am writing these lines.</p>\n",
      "<p>When this starts happening, the jobs of machine learning engineers will not disappearârather, engineers will move higher up the value creation chain. \n",
      "They will start putting a lot more effort into crafting complex loss functions that truly reflect business goals, \n",
      "and understanding deeply how their models impact the digital ecosystems in which they are deployed \n",
      "(e.g. the users that consume the model's predictions and generate the model's training data)\n",
      "âproblems that currently only the largest company can afford to consider.</p>\n",
      "<h2>Lifelong learning and modular subroutine reuse</h2>\n",
      "<p>If models get more complex and are built on top of richer algorithmic primitives, then this increased complexity will require higher \n",
      "reuse between tasks, rather than training a new model from scratch every time we have a new task or a new dataset. Indeed, a lot datasets \n",
      "would not contain enough information to develop a new complex model from scratch, and it will become necessary to leverage information \n",
      "coming from previously encountered datasets. Much like you don't learn English from scratch every time you open a new bookâthat would be \n",
      "impossible. Besides, training models from scratch on every new task is very inefficient due to the large overlap between the current tasks and \n",
      "previously encountered tasks.</p>\n",
      "<p>Additionally, a remarkable observation that has been made repeatedly in recent years is that training a <em>same</em> model to do several loosely \n",
      "connected tasks at the same time results in a model that is <em>better at each task</em>. For instance, training a same neural machine translation \n",
      "model to cover both English-to-German translation and French-to-Italian translation will result in a model that is better at each language \n",
      "pair. Training an image classification model jointly with an image segmentation model, sharing the same convolutional base, results in a \n",
      "model that is better at both tasks. And so on. This is fairly intuitive: there is always <em>some</em> information overlap between these seemingly \n",
      "disconnected tasks, and the joint model has thus access to a greater amount of information about each individual task than a model trained \n",
      "on that specific task only.</p>\n",
      "<p>What we currently do along the lines of model reuse across tasks is to leverage pre-trained weights for models that perform common \n",
      "functions, like visual feature extraction. You saw this in action in Chapter 5. In the future, I would expect a generalized version of this \n",
      "to be commonplace: we would not only leverage previously learned features (submodel weights), but also model architectures and training \n",
      "procedures. As models become more like programs, we would start reusing <em>program subroutines</em>, like the functions and classes found in \n",
      "human programming languages.</p>\n",
      "<p>Think of the process of software development today: once an engineer solves a specific problem (HTTP queries in Python, for instance), they \n",
      "will package it as an abstract and reusable library. Engineers that face a similar problem in the future can simply search for existing \n",
      "libraries, download one and use it in their own project. In a similar way, in the future, meta-learning systems will be able to assemble \n",
      "new programs by sifting through a global library of high-level reusable blocks. When the system would find itself developing similar \n",
      "program subroutines for several different tasks, if would come up with an \"abstract\", reusable version of the subroutine and would store it \n",
      "in the global library. Such a process would implement the capability for <em>abstraction</em>, a necessary component for achieving \"extreme \n",
      "generalization\": a subroutine that is found to be useful across different tasks and domains can be said to \"abstract\" some aspect of problem-solving. \n",
      "This definition of \"abstraction\" is similar to the notion of abstraction in software engineering. \n",
      "These subroutines could be either geometric (deep learning modules with pre-trained representations) \n",
      "or algorithmic (closer to the libraries that contemporary software engineers manipulate).</p>\n",
      "<p><img alt=\"A meta-learner capable of quickly developing task-specific models using reusable primitives (both algorithmic and geometric), thus achieving &quot;extreme generalization&quot;.\" src=\"/img/future-of-dl/metalearning2.png\" /></p>\n",
      "<p><strong>Figure:</strong> <em>A meta-learner capable of quickly developing task-specific models using reusable primitives (both algorithmic and geometric), thus achieving \"extreme generalization\".</em></p>\n",
      "<h2>In summary: the long-term vision</h2>\n",
      "<p>In short, here is my long-term vision for machine learning:</p>\n",
      "<ul>\n",
      "<li>Models will be more like programs, and will have capabilities that go far beyond the continuous geometric transformations of the input \n",
      "data that we currently work with. These programs will arguably be much closer to the abstract mental models that humans maintain about their \n",
      "surroundings and themselves, and they will be capable of stronger generalization due to their rich algorithmic nature.</li>\n",
      "<li>In particular, models will blend <em>algorithmic modules</em> providing formal reasoning, search, and abstraction capabilities, with <em>geometric modules</em> \n",
      "providing informal intuition and pattern recognition capabilities. \n",
      "AlphaGo (a system that required a lot of manual software engineering and human-made design decisions) \n",
      "provides an early example of what such a blend between symbolic and geometric AI could look like.</li>\n",
      "<li>They will be <em>grown</em> automatically rather than handcrafted by human engineers, using modular parts stored in a global library of reusable \n",
      "subroutinesâa library evolved by learning high-performing models on thousands of previous tasks and datasets. As common problem-solving \n",
      "patterns are identified by the meta-learning system, they would be turned into a reusable subroutineâmuch like functions and classes in \n",
      "contemporary software engineeringâand added to the global library. This achieves the capability for <em>abstraction</em>.</li>\n",
      "<li>This global library and associated model-growing system will be able to achieve some form of human-like \"extreme generalization\": given a \n",
      "new task, a new situation, the system would be able to assemble a new working model appropriate for the task using very little data, thanks \n",
      "to 1) rich program-like primitives that generalize well and 2) extensive experience with similar tasks. \n",
      "In the same way that humans can learn to play a complex new video \n",
      "game using very little play time because they have experience with many previous games, and because the models derived from this previous \n",
      "experience are abstract and program-like, rather than a basic mapping between stimuli and action.</li>\n",
      "<li>As such, this perpetually-learning model-growing system could be interpreted as an AGIâan Artificial General Intelligence. But don't \n",
      "expect any singularitarian robot apocalypse to ensue: that's a pure fantasy, coming from a long series of profound misunderstandings of \n",
      "both intelligence and technology. This critique, however, does not belong here.</li>\n",
      "</ul>\n",
      "<p><em><a href=\"https://twitter.com/fchollet\">@fchollet</a>, May 2017</em></p>\n",
      "        </div><!-- /.entry-content -->\n",
      "\n",
      "</article>\n",
      "</section>\n",
      "\n",
      "        <footer id=\"footer\" class=\"body\">\n",
      "                <address id=\"about\" class=\"vcard body\">\n",
      "                Powered by <a href=\"http://alexis.notmyidea.org/pelican/\">pelican</a>, which takes great advantages of <a href=\"http://python.org\">python</a>.\n",
      "                </address><!-- /#about -->\n",
      "        </footer><!-- /#footer -->\n",
      "\n",
      "    <script type=\"text/javascript\">\n",
      "    var gaJsHost = ((\"https:\" == document.location.protocol) ? \"https://ssl.\" : \"http://www.\");\n",
      "    document.write(unescape(\"%3Cscript src='\" + gaJsHost + \"google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E\"));\n",
      "    </script>\n",
      "    <script type=\"text/javascript\">\n",
      "    try {\n",
      "        var pageTracker = _gat._getTracker(\"UA-61785484-1\");\n",
      "    pageTracker._trackPageview();\n",
      "    } catch(err) {}</script>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "from collections import Counter\n",
    "\n",
    "link = \"https://blog.keras.io/the-future-of-deep-learning.html\"\n",
    "\n",
    "html = requests.get(link).text\n",
    "content = ''\n",
    "is_html_code = 0\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stripping the string form unnecessary characters and HTML code was challenging. I ended up creating a loop which ignores everything between the <> brackets, operating kind of like a state/machine. The < character switches the 'is_html_code' variable into 1 which blocks registration of new characters until the closing bracket has arrived. The registration method uses an isalpha() funtion to filter out any non-alpha characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thefutureofdeeplearningthekerasblogkerasisadeeplearninglibraryforpythonthatissimplemodularandextensiblearchivesgithubdocumentationgooglegroupthefutureofdeeplearningtuejulybyfrancoischolletinessaysthispostisadaptedfromsectionofchapterofmybookdeeplearningwithpythonmanningpublicationsitispartofaseriesoftwopostsonthecurrentlimitationsofdeeplearninganditsfutureyoucanreadthefirstpartherethelimitationsofdeeplearninggivenwhatweknowofhowdeepnetsworkoftheirlimitationsandofthecurrentstateoftheresearchlandscapecanwepredictwherethingsareheadedinthemediumtermherearesomepurelypersonalthoughtsnotethatidonthaveacrystalballsoalotofwhatianticipatemightfailtobecomerealitythisisacompletelyspeculativepostiamsharingthesepredictionsnotbecauseiexpectthemtobeprovencompletelyrightinthefuturebutbecausetheyareinterestingandactionableinthepresentatahighlevelthemaindirectionsinwhichiseepromisearemodelsclosertogeneralpurposecomputerprogramsbuiltontopoffarricherprimitivesthanourcurrentdifferentiablelayersâthisishowwewillgettoreasoningandabstractionthefundamentalweaknessofcurrentmodelsnewformsoflearningthatmaketheabovepossibleâallowingmodelstomoveawayfromjustdifferentiabletransformsmodelsthatrequirelessinvolvementfromhumanengineersâitshouldntbeyourjobtotuneknobsendlesslygreatersystematicreuseofpreviouslylearnedfeaturesandarchitecturesmetalearningsystemsbasedonreusableandmodularprogramsubroutinesadditionallydonotethattheseconsiderationsarenotspecifictothesortofsupervisedlearningthathasbeenthebreadandbutterofdeeplearningsofarârathertheyareapplicabletoanyformofmachinelearningincludingunsupervisedselfsupervisedandreinforcementlearningitisnotfundamentallyimportantwhereyourlabelscomefromorwhatyourtraininglooplookslikethesedifferentbranchesofmachinelearningarejustdifferentfacetsofasameconstructletsdiveinmodelsasprogramsaswenotedinourpreviouspostanecessarytransformationaldevelopmentthatwecanexpectinthefieldofmachinelearningisamoveawayfrommodelsthatperformpurelypatternrecognitionandcanonlyachievelocalgeneralizationtowardsmodelscapableofabstractionandreasoningthatcanachieveextremegeneralizationcurrentaiprogramsthatarecapableofbasicformsofreasoningareallhardcodedbyhumanprogrammersforinstancesoftwarethatreliesonsearchalgorithmsgraphmanipulationformallogicindeepmindsalphagoforexamplemostoftheintelligenceondisplayisdesignedandhardcodedbyexpertprogrammersegmontecarlotreesearchlearningfromdataonlyhappensinspecializedsubmodulesvaluenetworksandpolicynetworksbutinthefuturesuchaisystemsmaywellbefullylearnedwithnohumaninvolvementwhatcouldbethepathtomakethishappenconsiderawellknowntypeofnetworkrnnsimportantlyrnnshaveslightlylesslimitationsthanfeedforwardnetworksthatisbecausernnsareabitmorethanameregeometrictransformationtheyaregeometrictransformationsrepeatedlyappliedinsideaforloopthetemporalforloopisitselfhardcodedbyhumandevelopersitisabuiltinassumptionofthenetworknaturallyrnnsarestillextremelylimitedinwhattheycanrepresentprimarilybecauseeachsteptheyperformisstilljustadifferentiablegeometrictransformationandthewaytheycarryinformationfromsteptostepisviapointsinacontinuousgeometricspacestatevectorsnowimagineneuralnetworksthatwouldbeaugmentedinasimilarwaywithprogrammingprimitivessuchasforloopsâbutnotjustasinglehardcodedforloopwithahardcodedgeometricmemoryratheralargesetofprogrammingprimitivesthatthemodelwouldbefreetomanipulatetoexpanditsprocessingfunctionsuchasifbrancheswhilestatementsvariablecreationdiskstorageforlongtermmemorysortingoperatorsadvanceddatastructureslikelistsgraphsandhashtablesandmanymorethespaceofprogramsthatsuchanetworkcouldrepresentwouldbefarbroaderthanwhatcanberepresentedwithcurrentdeeplearningmodelsandsomeoftheseprogramscouldachievesuperiorgeneralizationpowerinawordwewillmoveawayfromhavingononehandhardcodedalgorithmicintelligencehandcraftedsoftwareandontheotherhandlearnedgeometricintelligencedeeplearningwewillhaveinsteadablendofformalalgorithmicmodulesthatprovidereasoningandabstractioncapabilitiesandgeometricmodulesthatprovideinformalintuitionandpatternrecognitioncapabilitiesthewholesystemwouldbelearnedwithlittleornohumaninvolvementarelatedsubfieldofaithatithinkmaybeabouttotakeoffinabigwayisthatofprogramsynthesisinparticularneuralprogramsynthesisprogramsynthesisconsistsinautomaticallygeneratingsimpleprogramsbyusingasearchalgorithmpossiblygeneticsearchasingeneticprogrammingtoexplorealargespaceofpossibleprogramsthesearchstopswhenaprogramisfoundthatmatchestherequiredspecificationsoftenprovidedasasetofinputoutputpairsasyoucanseeisithighlyreminiscentofmachinelearninggiventrainingdataprovidedasinputoutputpairswefindaprogramthatmatchesinputstooutputsandcangeneralizetonewinputsthedifferenceisthatinsteadoflearningparametervaluesinahardcodedprogramaneuralnetworkwegeneratesourcecodeviaadiscretesearchprocessiwoulddefinitelyexpectthissubfieldtoseeawaveofrenewedinterestinthenextfewyearsinparticulariwouldexpecttheemergenceofacrossoversubfieldinbetweendeeplearningandprogramsynthesiswherewewouldnotquitebegeneratingprogramsinageneralpurposelanguagebutratherwherewewouldbegeneratingneuralnetworksgeometricdataprocessingflowsaugmentedwitharichsetofalgorithmicprimitivessuchasforloopsâandmanyothersthisshouldbefarmoretractableandusefulthandirectlygeneratingsourcecodeanditwoulddramaticallyexpandthescopeofproblemsthatcanbesolvedwithmachinelearningâthespaceofprogramsthatwecangenerateautomaticallygivenappropriatetrainingdataablendofsymbolicaiandgeometricaicontemporaryrnnscanbeseenasaprehistoricancestortosuchhybridalgorithmicgeometricmodelsfigurealearnedprogramrelyingonbothgeometricprimitivespatternrecognitionintuitionandalgorithmicprimitivesreasoningsearchmemorybeyondbackpropagationanddifferentiablelayersifmachinelearningmodelsbecomemorelikeprogramsthentheywillmostlynolongerbedifferentiableâcertainlytheseprogramswillstillleveragecontinuousgeometriclayersassubroutineswhichwillbedifferentiablebutthemodelasawholewouldnotbeasaresultusingbackpropagationtoadjustweightvaluesinafixedhardcodednetworkcannotbethemethodofchoicefortrainingmodelsinthefutureâatleastitcannotbethewholestoryweneedtofigureouttotrainnondifferentiablesystemsefficientlycurrentapproachesincludegeneticalgorithmsevolutionstrategiescertainreinforcementlearningmethodsandadmmalternatingdirectionmethodofmultipliersnaturallygradientdescentisnotgoinganywhereâgradientinformationwillalwaysbeusefulforoptimizingdifferentiableparametricfunctionsbutourmodelswillcertainlybecomeincreasinglymoreambitiousthanmeredifferentiableparametricfunctionsandthustheirautomaticdevelopmentthelearninginmachinelearningwillrequiremorethanbackpropagationbesidesbackpropagationisendtoendwhichisagreatthingforlearninggoodchainedtransformationsbutisrathercomputationallyinefficientsinceitdoesntfullyleveragethemodularityofdeepnetworkstomakesomethingmoreefficientthereisoneuniversalrecipeintroducemodularityandhierarchysowecanmakebackpropitselfmoreefficientbyintroducingdecoupledtrainingmoduleswithsomesynchronizationmechanismbetweenthemorganizedinahierarchicalfashionthisstrategyissomewhatreflectedindeepmindsrecentworkonsyntheticgradientsiwouldexpectmoremoreworkalongtheselinesinthenearfutureonecanimagineafuturewheremodelsthatwouldbegloballynondifferentiablebutwouldfeaturedifferentiablepartswouldbetrainedâgrownâusinganefficientsearchprocessthatwouldnotleveragegradientswhilethedifferentiablepartswouldbetrainedevenfasterbytakingadvantageofgradientsusingsomemoreefficientversionofbackpropagationautomatedmachinelearninginthefuturemodelarchitectureswillbelearnedratherthanhandcraftedbyengineerartisanslearningarchitecturesautomaticallygoeshandinhandwiththeuseofrichersetsofprimitivesandprogramlikemachinelearningmodelscurrentlymostofthejobofadeeplearningengineerconsistsinmungingdatawithpythonscriptsthenlengthilytuningthearchitectureandhyperparametersofadeepnetworktogetaworkingmodelâoreventogettoastateoftheartmodeliftheengineerissoambitiousneedlesstosaythatisnotanoptimalsetupbutaicanhelptheretoounfortunatelythedatamungingpartistoughtoautomatesinceitoftenrequiresdomainknowledgeaswellasaclearhighlevelunderstandingofwhattheengineerwantstoachievehyperparametertuninghoweverisasimplesearchprocedureandwealreadyknowwhattheengineerwantstoachieveinthiscaseitisdefinedbythelossfunctionofthenetworkbeingtuneditisalreadycommonpracticetosetupbasicautomlsystemsthatwilltakecareofmostofthemodelknobtuningievensetupmyownyearsagotowinkagglecompetitionsatthemostbasiclevelsuchasystemwouldsimplytunethenumberoflayersinastacktheirorderandthenumberofunitsorfiltersineachlayerthisiscommonlydonewithlibrariessuchashyperoptwhichwediscussedinchapternoteofdeeplearningwithpythonbutwecanalsobefarmoreambitiousandattempttolearnanappropriatearchitecturefromscratchwithasfewconstraintsaspossiblethisispossibleviareinforcementlearningforinstanceorgeneticalgorithmsanotherimportantautomldirectionistolearnmodelarchitecturejointlywithmodelweightsbecausetraininganewmodelfromscratcheverytimewetryaslightlydifferentarchitectureistremendouslyinefficientatrulypowerfulautomlsystemwouldmanagetoevolvearchitecturesatthesametimeasthefeaturesofthemodelarebeingtunedviabackproponthetrainingdatathuseliminatingallcomputationalredundancysuchapproachesarealreadystartingtoemergeasiamwritingtheselineswhenthisstartshappeningthejobsofmachinelearningengineerswillnotdisappearâratherengineerswillmovehigherupthevaluecreationchaintheywillstartputtingalotmoreeffortintocraftingcomplexlossfunctionsthattrulyreflectbusinessgoalsandunderstandingdeeplyhowtheirmodelsimpactthedigitalecosystemsinwhichtheyaredeployedegtheusersthatconsumethemodelspredictionsandgeneratethemodelstrainingdataâproblemsthatcurrentlyonlythelargestcompanycanaffordtoconsiderlifelonglearningandmodularsubroutinereuseifmodelsgetmorecomplexandarebuiltontopofricheralgorithmicprimitivesthenthisincreasedcomplexitywillrequirehigherreusebetweentasksratherthantraininganewmodelfromscratcheverytimewehaveanewtaskoranewdatasetindeedalotdatasetswouldnotcontainenoughinformationtodevelopanewcomplexmodelfromscratchanditwillbecomenecessarytoleverageinformationcomingfrompreviouslyencountereddatasetsmuchlikeyoudontlearnenglishfromscratcheverytimeyouopenanewbookâthatwouldbeimpossiblebesidestrainingmodelsfromscratchoneverynewtaskisveryinefficientduetothelargeoverlapbetweenthecurrenttasksandpreviouslyencounteredtasksadditionallyaremarkableobservationthathasbeenmaderepeatedlyinrecentyearsisthattrainingasamemodeltodoseverallooselyconnectedtasksatthesametimeresultsinamodelthatisbetterateachtaskforinstancetrainingasameneuralmachinetranslationmodeltocoverbothenglishtogermantranslationandfrenchtoitaliantranslationwillresultinamodelthatisbetterateachlanguagepairtraininganimageclassificationmodeljointlywithanimagesegmentationmodelsharingthesameconvolutionalbaseresultsinamodelthatisbetteratbothtasksandsoonthisisfairlyintuitivethereisalwayssomeinformationoverlapbetweentheseseeminglydisconnectedtasksandthejointmodelhasthusaccesstoagreateramountofinformationabouteachindividualtaskthanamodeltrainedonthatspecifictaskonlywhatwecurrentlydoalongthelinesofmodelreuseacrosstasksistoleveragepretrainedweightsformodelsthatperformcommonfunctionslikevisualfeatureextractionyousawthisinactioninchapterinthefutureiwouldexpectageneralizedversionofthistobecommonplacewewouldnotonlyleveragepreviouslylearnedfeaturessubmodelweightsbutalsomodelarchitecturesandtrainingproceduresasmodelsbecomemorelikeprogramswewouldstartreusingprogramsubroutineslikethefunctionsandclassesfoundinhumanprogramminglanguagesthinkoftheprocessofsoftwaredevelopmenttodayonceanengineersolvesaspecificproblemhttpqueriesinpythonforinstancetheywillpackageitasanabstractandreusablelibraryengineersthatfaceasimilarprobleminthefuturecansimplysearchforexistinglibrariesdownloadoneanduseitintheirownprojectinasimilarwayinthefuturemetalearningsystemswillbeabletoassemblenewprogramsbysiftingthroughagloballibraryofhighlevelreusableblockswhenthesystemwouldfinditselfdevelopingsimilarprogramsubroutinesforseveraldifferenttasksifwouldcomeupwithanabstractreusableversionofthesubroutineandwouldstoreitinthegloballibrarysuchaprocesswouldimplementthecapabilityforabstractionanecessarycomponentforachievingextremegeneralizationasubroutinethatisfoundtobeusefulacrossdifferenttasksanddomainscanbesaidtoabstractsomeaspectofproblemsolvingthisdefinitionofabstractionissimilartothenotionofabstractioninsoftwareengineeringthesesubroutinescouldbeeithergeometricdeeplearningmoduleswithpretrainedrepresentationsoralgorithmicclosertothelibrariesthatcontemporarysoftwareengineersmanipulatefigureametalearnercapableofquicklydevelopingtaskspecificmodelsusingreusableprimitivesbothalgorithmicandgeometricthusachievingextremegeneralizationinsummarythelongtermvisioninshorthereismylongtermvisionformachinelearningmodelswillbemorelikeprogramsandwillhavecapabilitiesthatgofarbeyondthecontinuousgeometrictransformationsoftheinputdatathatwecurrentlyworkwiththeseprogramswillarguablybemuchclosertotheabstractmentalmodelsthathumansmaintainabouttheirsurroundingsandthemselvesandtheywillbecapableofstrongergeneralizationduetotheirrichalgorithmicnatureinparticularmodelswillblendalgorithmicmodulesprovidingformalreasoningsearchandabstractioncapabilitieswithgeometricmodulesprovidinginformalintuitionandpatternrecognitioncapabilitiesalphagoasystemthatrequiredalotofmanualsoftwareengineeringandhumanmadedesigndecisionsprovidesanearlyexampleofwhatsuchablendbetweensymbolicandgeometricaicouldlookliketheywillbegrownautomaticallyratherthanhandcraftedbyhumanengineersusingmodularpartsstoredinagloballibraryofreusablesubroutinesâalibraryevolvedbylearninghighperformingmodelsonthousandsofprevioustasksanddatasetsascommonproblemsolvingpatternsareidentifiedbythemetalearningsystemtheywouldbeturnedintoareusablesubroutineâmuchlikefunctionsandclassesincontemporarysoftwareengineeringâandaddedtothegloballibrarythisachievesthecapabilityforabstractionthisgloballibraryandassociatedmodelgrowingsystemwillbeabletoachievesomeformofhumanlikeextremegeneralizationgivenanewtaskanewsituationthesystemwouldbeabletoassembleanewworkingmodelappropriateforthetaskusingverylittledatathankstorichprogramlikeprimitivesthatgeneralizewellandextensiveexperiencewithsimilartasksinthesamewaythathumanscanlearntoplayacomplexnewvideogameusingverylittleplaytimebecausetheyhaveexperiencewithmanypreviousgamesandbecausethemodelsderivedfromthispreviousexperienceareabstractandprogramlikeratherthanabasicmappingbetweenstimuliandactionassuchthisperpetuallylearningmodelgrowingsystemcouldbeinterpretedasanagiâanartificialgeneralintelligencebutdontexpectanysingularitarianrobotapocalypsetoensuethatsapurefantasycomingfromalongseriesofprofoundmisunderstandingsofbothintelligenceandtechnologythiscritiquehoweverdoesnotbelongherefcholletmaypoweredbypelicanwhichtakesgreatadvantagesofpythonvargajshosthttpsdocumentlocationprotocolhttpssslhttpwwwdocumentwriteunescapecscriptsrcgajshostgoogleanalyticscomgajstypetextjavascriptecscriptetryvarpagetrackergatgettrackeruapagetrackertrackpageviewcatcherr\n"
     ]
    }
   ],
   "source": [
    "for character in html:\n",
    "    if character is '<':\n",
    "        is_html_code = 1\n",
    "    if is_html_code is 0 and character.isalpha():\n",
    "        content = content + character.lower()\n",
    "    if character is '>':\n",
    "        is_html_code = 0\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Counter function form collections, it was very easy to count every letter in the stripped string. Pandas' DataFrame helps a lot here, I was able to make a histogram with the correct labels very quickly and easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "letter_count = Counter(content)\n",
    "data = pandas.DataFrame.from_dict(letter_count, orient='index')\n",
    "data.plot(kind='bar')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
